# 作业一：LLM实现与微调 实验报告
#### 杨浩 2100012857

## 1. Tokenization
#### 1.1.1 简述BPE算法和基于BPE算法训练LLM tokenizer的流程。
 **BPE 算法**：
- **初始化**：将文本数据拆分为最小的基本单元（通常是字符级或字节级）。
- **统计频率**：计算所有相邻的字符（或子词）对的出现频率。
- **合并频率最高的对子**：找到文本中出现次数最多的相邻子词对，并将其合并为一个新的子词。
- **更新词表**：用新合并的子词替换文本中所有对应的子词对，并更新词表。
- **重复步骤 2-4**：持续执行合并操作，直到达到预设的合并次数（merge operations）或词表大小（vocab_size）上限。
  
 **基于BPE训练LLM tokenizer流程**
- **数据预处理**：收集大规模文本数据，进行数据清洗与整理。
- **初始化词汇表**：词汇表最初包含所有单字符单位，并可以额外添加一些特殊token
- **训练BPE词汇表**：基于BPE算法训练模型，得到词表和合并顺序表
- **生成Tokenizer模型**：基于得到的词表即可用于tokenizer的encode和decode

#### 1.1.2 实现一个基于BPE算法的tokenizer
具体实现方式已详细注明在下面代码中。
```
def __init__(self):
    # 存储decode词表，key：token；value：idx
    self.vocab2id = {}
    # 存储词表，key：idx；value：token
    self.id2vocab = {}
    # 存储合并顺序
    self.merge_list = []
    # 参考了GPT2的实现，为避免每次找出现最多的merge_pair都遍历所有token，我们可以先分词（模板借用了GPT2的PAT模式），然后在单词的范围里找不同的pair，计出现次数时加count[word]的频率即可，在重复单词多时可有效减少每次需遍历的token数
    self.PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    # 初始化词表
    for i in range(256):
        self.id2vocab[i] = bytes([i])

# 如上所述，先分词，然后再转化为'utf-8'形式的tokens，再使用BPE算法训练
def train(self, text, vocab_size):
    pass

# 直接转变为utf-8的tokens，再使用merge_list顺序进行merge
def encode(self, text):
    pass

# 直接转化再拼接即可
def decode(self, ids):
    pass
```
**Q: 训练你的tokenizer，vocab_size为1024**
训练文件为`train.py`，设置了训练后存储相关参数方便再次调用。
**Q: encode再decode manual.txt，检查与原始manual.txt是否完全一致？**
生成文件为`test_manual.txt`，确实一致。
**Q: 学习使用huggingface transformers中的tokenizer，分别encode以下句子，比较两者的输出，简要解释长度上和具体token上不同的原因是什么。**
本题用的文件为`test.py`，对于文本1和文本2，分别调用gpt2和自己实现的tokenizer的对比：
| 模型\长度  | 文本1（英文为主） | 文本2（中文为主） |
|---|---|---|
| **GPT2的tokenizer** | 185|957|
| **自己实现的tokenizer**|306|232|

**文本1**：GPT2模型encode后的长度明显更短，一个很大的原因是词汇表的大小，在GPT2模型encode的文字中，出现了像47824、45780等这样较大的idx，说明其vocab_size远超过我们训练的1024的大小；另一个重要的原因是我们的训练语库`manual.txt`几乎为全中文，故对于英文的常见pair并不能有效的组合在一起，而GPT2却能高效合并常见英文单词和子词，如 "University" → "Uni" + "versity"。
**文本2**：相反，对于中文测试语句我自己实现的模型encode后更短，这是因为测试语句和我训练的语料库所述内容相关，词汇表虽小（1024），但高度相关，能够很好的合并高频词；而GPT2的训练语库内容更多，普适性更好，但测试语句的相关程度上并没有我自己训练的这么高（都是关于研究生、博士等等），一些在此语境出现的高频词可能在大语料库里面出现并不频繁，所以相对来说会稍长一些。
#### 1.2 回答问题
**Q：Python中使用什么函数查看字符的Unicode，什么函数将Unicode转换成字符？并使用它们查看“北”“大”的Unicode，查看Unicode为22823、27169、22411对应的字符。**
用`ord()`函数查看unicode，用`chr()`转化为字符；`北`的Unicode为`21271`，`大`的Unicode为`22823`，`22823`、`27169`、`22411`对应的字符分别为`大`、`模`、`型`。
**Q：Tokenizer的vocab size大和小分别有什么好处和坏处？**
|Vocab Size|优点|缺点|
|---|---|---|
|大（如 50k）|减少常见词的拆分（token 数量少），对高频词处理更高效。|模型参数更多，训练和推理更慢。|
|小（如 1k）|模型更轻量。|常见词被过度拆分（token 数量多），可能丢失语义信息。|

**Q：为什么 LLM 不能处理非常简单的字符串操作任务，比如反转字符串？**
1. LLM 基于概率生成文本，而非执行算法。
2. 训练数据限制：反转字符串的逻辑在训练数据中极少出现（文本中几乎没有反转后的对应示例）。
3. 自回归生成的局限性：decode-only LLM逐token生成，无法全局规划（如反向遍历字符串）

**Q：为什么 LLM 在非英语语言（例如日语）上表现较差？**
1. 数据量不足：训练数据以英文为主，非英语语料占比低，分词效率低。
2. 语言结构差异：日语等黏着语的语法与英语差异大，模型难以迁移学习。

**Q：为什么 LLM 在简单算术问题上表现不好？**
1. LLM依赖统计模式，而非符号计算。
2. 算术问题的正确解在数据中可能被错误答案污染，且很难学到计算模型（最多记住结果）。
3. LLM如GPT2将数字视为字符串，数字比较长时可能会被错误分词。
   
**Q：为什么 GPT-2 在编写 Python 代码时遇到比预期更多的困难？**
1. 训练数据侧重自然语言：代码语料占比低，且未针对代码结构优化。
2. 代码需要严格的语法和逻辑，基于概率生成的LLM易生成表面合理但无法运行的代码，因为基于概率的LLM易在细节上出错（如漏掉冒号 :），而python敏感。

**Q：为什么 LLM 遇到字符串 “<|endoftext|>” 时会突然中断？**
因为在LLM如GPT2中设计tokenizer时会有一个`special_tokens`的参数，这部分会进行单独的分词，用于预训练时用于标记文本结束的Token，学到模式遇到此Token模型则停止生成。

**Q：为什么当问 LLM 关于 “SolidGoldMagikarp” 的问题时 LLM 会崩溃？**
1. `SolidGoldMagikarp`可能是某一数据集来源中的一个用户名或高频词汇，在那特定语境中高频出现（如该用户多次发帖），导致在训练数据中出现频率较高，但语义上无意义，但是被Tokenizer映射为一个独立的Token，但由于缺乏语义（该用户发的贴子内容和用户名均无关系），导致模型在生成时可能随机输出或触发错误处理逻辑（猜测如概率都很小到无法选词之类）。
2. 检索时发现模型甚至可能无法复述该词，可能因为该token位于token嵌入空间的中心附近。这意味着，模型在区分这些token和其他位于嵌入空间中心附近的token时存在困难，因此当被要求“重复”这些token时，模型会选择错误的token。

**Q：为什么在使用 LLM 时应该更倾向于使用 YAML 而不是 JSON？**
1. 可读性：YAML的缩进结构更接近自然语言，便于模型生成。
2. 容错性：YAML允许省略引号和逗号，而 JSON 对语法要求严格。

**Q：为什么 LLM 实际上不是端到端的语言建模？**
1. 因为通常来说我们会对预训练好的LLM进行针对下游任务的后训练和推理（解码策略、温度调节、上下文管理等），可能还会因为引入Instruction Tuning、RLHF带来的非端到端。
2. LLM可能会结合外部知识源，如RAG提高回答的准确性，利用如Codex等等工具进行辅助代码生成或者数学计算。

## 2. LLM Implementation
#### 2.1 section 1
#### 2.2 section 2
#### 2.3 section 3
#### 2.4 section 4